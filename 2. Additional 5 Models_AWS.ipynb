{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114-HR-4322,4322,\"To clarify the prohibition on affiliation under the Mentor-Protege Program of the Department of Defense, to amend the Small Business Act to improve cooperation between the mentor-protege programs of the Small Business Administration and the Department of Defense, and for other purposes.\",15\n",
      "\n",
      "114-HR-435,435,\"To direct the Secretary of the Interior to sell certain Federal lands in Arizona, Colorado, Idaho, Montana, Nebraska, Nevada, New Mexico, Oregon, Utah, and Wyoming, previously identified as suitable for disposal, and for other purposes.\",21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import the data \n",
    "os.chdir(\"./data\")\n",
    "\n",
    "def read_data(file_name):\n",
    "    labels = []\n",
    "    titles = []\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                if 'BillID,' in line:\n",
    "                    pass\n",
    "                elif '\"' in line:\n",
    "                    split_1 = line.split('\"')\n",
    "                    label = split_1[-1].lstrip(\",\").rstrip(\"\\n\")\n",
    "                    labels.append(int(label))\n",
    "                    title = split_1[-2].lstrip(\",\")\n",
    "                    title = title.replace(\",\", \"\") #here I deleted \",\" from the titles.\n",
    "                    titles.append(title)\n",
    "\n",
    "                else:\n",
    "                    split = line.split(\",\")\n",
    "                    titles.append(split[-2])\n",
    "                    labels.append(int(split[-1].strip(\"\\n\")))\n",
    "        except:\n",
    "            print(line)\n",
    "                \n",
    "    return ((titles, labels))\n",
    "\n",
    "train_titles_raw, train_labels_raw = read_data(\"congress_train.csv\")\n",
    "val_titles_raw, val_labels_raw = read_data(\"congress_val.csv\")\n",
    "test_titles_raw, test_labels_raw = read_data(\"congress_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42197 unique tokens.\n",
      "Shape of data tensor: (274482, 100)\n",
      "Found 24985 unique tokens.\n",
      "Shape of data tensor: (69649, 100)\n",
      "Found 19565 unique tokens.\n",
      "Shape of data tensor: (37733, 100)\n"
     ]
    }
   ],
   "source": [
    "#setting up_data\n",
    "def setting_data(text_lst,maxlen, max_words):\n",
    "    maxlen = maxlen\n",
    "    max_words = max_words\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(text_lst)\n",
    "    sequences = tokenizer.texts_to_sequences(text_lst)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "    print('Shape of data tensor:', data.shape)\n",
    "    return (data)\n",
    "\n",
    "#preprocessing train data \n",
    "x_train = setting_data(train_titles_raw, \n",
    "                       maxlen = 100,\n",
    "                       max_words = 10000)\n",
    "\n",
    "y_train = to_categorical(train_labels_raw)\n",
    "\n",
    "#preprocessing validation data \n",
    "x_val = setting_data(val_titles_raw, \n",
    "                     maxlen = 100, \n",
    "                     max_words = 10000)\n",
    "\n",
    "y_val = to_categorical(val_labels_raw)\n",
    "\n",
    "\n",
    "#preprocessing test data \n",
    "x_test = setting_data(test_titles_raw, \n",
    "                      maxlen = 100,\n",
    "                      max_words = 10000)\n",
    "\n",
    "y_test = to_categorical(test_labels_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, GRU, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Estimate five additional neural network models with different configurations of hyperparameters (e.g. number of layers, number of hidden units, dropout, weight regularization, pre-trained word embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 100, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 24)                792       \n",
      "=================================================================\n",
      "Total params: 329,112\n",
      "Trainable params: 329,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 274482 samples, validate on 69649 samples\n",
      "Epoch 1/10\n",
      "274482/274482 [==============================] - 38s 138us/step - loss: 2.7496 - acc: 0.1733 - val_loss: 2.9960 - val_acc: 0.1463\n",
      "Epoch 2/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 2.0793 - acc: 0.3946 - val_loss: 3.1408 - val_acc: 0.1671\n",
      "Epoch 3/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 1.5293 - acc: 0.5811 - val_loss: 3.6323 - val_acc: 0.1442\n",
      "Epoch 4/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 1.1842 - acc: 0.6925 - val_loss: 3.9804 - val_acc: 0.1258\n",
      "Epoch 5/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.9694 - acc: 0.7534 - val_loss: 4.1868 - val_acc: 0.1391\n",
      "Epoch 6/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.8410 - acc: 0.7867 - val_loss: 4.2587 - val_acc: 0.1298\n",
      "Epoch 7/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.7593 - acc: 0.8058 - val_loss: 4.2970 - val_acc: 0.1221\n",
      "Epoch 8/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.7025 - acc: 0.8183 - val_loss: 4.2638 - val_acc: 0.1208\n",
      "Epoch 9/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.6592 - acc: 0.8273 - val_loss: 4.4006 - val_acc: 0.1263\n",
      "Epoch 10/10\n",
      "274482/274482 [==============================] - 35s 129us/step - loss: 0.6277 - acc: 0.8337 - val_loss: 4.4933 - val_acc: 0.1262\n"
     ]
    }
   ],
   "source": [
    "#Mod_1\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "mod_1 = Sequential()\n",
    "mod_1.add(Embedding(10000, 32, input_length = maxlen))\n",
    "mod_1.add(LSTM(32,\n",
    "          dropout=0.2,\n",
    "          recurrent_dropout=0.2))\n",
    "mod_1.add(Dense(24, activation='softmax'))\n",
    "\n",
    "mod_1.compile(optimizer='rmsprop', \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['acc'])\n",
    "mod_1.summary()\n",
    "\n",
    "mod_1_history = mod_1.fit(x_train, y_train, epochs= 10, batch_size= 2048,\n",
    "                         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 32)           320000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                792       \n",
      "=================================================================\n",
      "Total params: 327,032\n",
      "Trainable params: 327,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 274482 samples, validate on 69649 samples\n",
      "Epoch 1/10\n",
      "274482/274482 [==============================] - 29s 107us/step - loss: 2.8243 - acc: 0.1591 - val_loss: 2.8552 - val_acc: 0.1342\n",
      "Epoch 2/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 2.3504 - acc: 0.3197 - val_loss: 3.0461 - val_acc: 0.1257\n",
      "Epoch 3/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 1.8557 - acc: 0.4857 - val_loss: 3.4354 - val_acc: 0.1058\n",
      "Epoch 4/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 1.4553 - acc: 0.6207 - val_loss: 3.6795 - val_acc: 0.1005\n",
      "Epoch 5/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 1.1533 - acc: 0.7039 - val_loss: 3.8112 - val_acc: 0.1079\n",
      "Epoch 6/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 0.9473 - acc: 0.7563 - val_loss: 3.9904 - val_acc: 0.1100\n",
      "Epoch 7/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 0.8216 - acc: 0.7867 - val_loss: 4.0968 - val_acc: 0.1098\n",
      "Epoch 8/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 0.7471 - acc: 0.8023 - val_loss: 4.1562 - val_acc: 0.1084\n",
      "Epoch 9/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 0.6950 - acc: 0.8134 - val_loss: 4.1555 - val_acc: 0.1111\n",
      "Epoch 10/10\n",
      "274482/274482 [==============================] - 28s 103us/step - loss: 0.6582 - acc: 0.8206 - val_loss: 4.1891 - val_acc: 0.1114\n"
     ]
    }
   ],
   "source": [
    "#Mod_2\n",
    "maxlen = 100\n",
    "\n",
    "mod_2 = Sequential()\n",
    "mod_2.add(Embedding(10000, 32, input_length = maxlen))\n",
    "mod_2.add(GRU(32,\n",
    "          dropout=0.1,\n",
    "          recurrent_dropout=0.5))\n",
    "mod_2.add(Dense(24, activation='softmax'))\n",
    "\n",
    "mod_2.compile(optimizer='rmsprop', \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['acc'])\n",
    "mod_2.summary()\n",
    "\n",
    "mod_2_history = mod_2.fit(x_train, y_train, epochs= 10, batch_size= 2048,\n",
    "                         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "gru_61 (GRU)                 (None, 100, 16)           1584      \n",
      "_________________________________________________________________\n",
      "gru_62 (GRU)                 (None, 16)                1584      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 24)                408       \n",
      "=================================================================\n",
      "Total params: 163,576\n",
      "Trainable params: 163,576\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 274482 samples, validate on 69649 samples\n",
      "Epoch 1/10\n",
      "274482/274482 [==============================] - 56s 203us/step - loss: 2.8471 - acc: 0.1407 - val_loss: 2.9641 - val_acc: 0.1264\n",
      "Epoch 2/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 2.4980 - acc: 0.2033 - val_loss: 3.2019 - val_acc: 0.1065\n",
      "Epoch 3/10\n",
      "274482/274482 [==============================] - 52s 189us/step - loss: 2.3340 - acc: 0.2509 - val_loss: 3.3139 - val_acc: 0.1054\n",
      "Epoch 4/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 2.0870 - acc: 0.3315 - val_loss: 3.5321 - val_acc: 0.0925\n",
      "Epoch 5/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 1.8733 - acc: 0.4031 - val_loss: 3.7090 - val_acc: 0.0928\n",
      "Epoch 6/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 1.7008 - acc: 0.4704 - val_loss: 3.8218 - val_acc: 0.0957\n",
      "Epoch 7/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 1.5691 - acc: 0.5228 - val_loss: 4.0122 - val_acc: 0.0996\n",
      "Epoch 8/10\n",
      "274482/274482 [==============================] - 52s 189us/step - loss: 1.4591 - acc: 0.5693 - val_loss: 4.1265 - val_acc: 0.0995\n",
      "Epoch 9/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 1.3720 - acc: 0.6042 - val_loss: 4.2610 - val_acc: 0.0894\n",
      "Epoch 10/10\n",
      "274482/274482 [==============================] - 52s 188us/step - loss: 1.2969 - acc: 0.6354 - val_loss: 4.2842 - val_acc: 0.0981\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Reshape\n",
    "#Mod_3\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "mod_3 = Sequential()\n",
    "mod_3.add(Embedding(10000, 16, input_length = maxlen))\n",
    "mod_3.add(GRU(16, dropout=0.2, \n",
    "              recurrent_dropout=0.3,\n",
    "              return_sequences=True))\n",
    "mod_3.add(GRU(16, dropout=0.2,\n",
    "             recurrent_dropout=0.3))\n",
    "mod_3.add(Dense(24, activation='softmax'))\n",
    "\n",
    "mod_3.compile(optimizer='rmsprop', \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['acc'])\n",
    "mod_3.summary()\n",
    "\n",
    "mod_3_history = mod_3.fit(x_train, y_train, epochs= 10, batch_size= 2048,\n",
    "                         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_31 (Embedding)     (None, 100, 24)           240000    \n",
      "_________________________________________________________________\n",
      "gru_64 (GRU)                 (None, 100, 24)           3528      \n",
      "_________________________________________________________________\n",
      "gru_65 (GRU)                 (None, 100, 24)           3528      \n",
      "_________________________________________________________________\n",
      "gru_66 (GRU)                 (None, 24)                3528      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 24)                600       \n",
      "=================================================================\n",
      "Total params: 251,184\n",
      "Trainable params: 251,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 274482 samples, validate on 69649 samples\n",
      "Epoch 1/10\n",
      "274482/274482 [==============================] - 85s 309us/step - loss: 2.7242 - acc: 0.1589 - val_loss: 3.1554 - val_acc: 0.1124\n",
      "Epoch 2/10\n",
      "274482/274482 [==============================] - 80s 290us/step - loss: 2.3342 - acc: 0.2296 - val_loss: 3.4800 - val_acc: 0.1004\n",
      "Epoch 3/10\n",
      "274482/274482 [==============================] - 80s 290us/step - loss: 2.0071 - acc: 0.3629 - val_loss: 3.8832 - val_acc: 0.0942\n",
      "Epoch 4/10\n",
      "274482/274482 [==============================] - 80s 290us/step - loss: 1.7081 - acc: 0.4920 - val_loss: 4.1851 - val_acc: 0.1029\n",
      "Epoch 5/10\n",
      "274482/274482 [==============================] - 79s 289us/step - loss: 1.5001 - acc: 0.5715 - val_loss: 4.3448 - val_acc: 0.1034\n",
      "Epoch 6/10\n",
      "274482/274482 [==============================] - 79s 289us/step - loss: 1.3725 - acc: 0.6204 - val_loss: 4.4534 - val_acc: 0.0956\n",
      "Epoch 7/10\n",
      "274482/274482 [==============================] - 79s 289us/step - loss: 1.2791 - acc: 0.6528 - val_loss: 4.4501 - val_acc: 0.0958\n",
      "Epoch 8/10\n",
      "274482/274482 [==============================] - 79s 289us/step - loss: 1.2088 - acc: 0.6768 - val_loss: 4.5339 - val_acc: 0.0814\n",
      "Epoch 9/10\n",
      "274482/274482 [==============================] - 79s 290us/step - loss: 1.1499 - acc: 0.6956 - val_loss: 4.5457 - val_acc: 0.0863\n",
      "Epoch 10/10\n",
      "274482/274482 [==============================] - 79s 289us/step - loss: 1.0990 - acc: 0.7110 - val_loss: 4.4637 - val_acc: 0.0866\n"
     ]
    }
   ],
   "source": [
    "#Mod_4\n",
    "#Mod_4\n",
    "maxlen = 100\n",
    "\n",
    "mod_4 = Sequential()\n",
    "mod_4.add(Embedding(10000, 24, input_length = maxlen))\n",
    "mod_4.add(GRU(24,\n",
    "          dropout=0.1,\n",
    "          recurrent_dropout=0.3,\n",
    "          return_sequences=True))\n",
    "mod_4.add(GRU(24,\n",
    "          dropout=0.1,\n",
    "          recurrent_dropout=0.3,\n",
    "          return_sequences=True))\n",
    "mod_4.add(GRU(24,\n",
    "          dropout=0.3,\n",
    "          recurrent_dropout=0.7))\n",
    "mod_4.add(Dense(24, activation='softmax'))\n",
    "\n",
    "mod_4.compile(optimizer='rmsprop', \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['acc'])\n",
    "mod_4.summary()\n",
    "\n",
    "mod_4_history = mod_4.fit(x_train, y_train, epochs= 10, batch_size= 2048,\n",
    "                         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mod_5\n",
    "maxlen = 100\n",
    "\n",
    "mod_5 = Sequential()\n",
    "mod_5.add(Embedding(10000, 8, input_length = maxlen))\n",
    "mod_5.add(LSTM(8,\n",
    "          dropout=0.3,\n",
    "          recurrent_dropout=0.1, \n",
    "          return_sequences=True))\n",
    "mod_5.add(LSTM(8,\n",
    "          dropout=0.3,\n",
    "          recurrent_dropout=0.1, \n",
    "          return_sequences=True))\n",
    "mod_5.add(LSTM(8,\n",
    "          dropout=0.3,\n",
    "          recurrent_dropout=0.1))\n",
    "mod_5.add(Dense(24, activation='softmax'))\n",
    "\n",
    "mod_5.compile(optimizer='rmsprop', \n",
    "                   loss='categorical_crossentropy', \n",
    "                   metrics=['acc'])\n",
    "mod_5.summary()\n",
    "\n",
    "mod_5_history = mod_5.fit(x_train, y_train, epochs= 10, batch_size= 1024,\n",
    "                         validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "history_lst = [mod_1_history, mod_2_history, mod_3_history, mod_4_history]\n",
    "pickle.dump(history_lst, open(\"history_lst_2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best performing model based on the validation set and evaluate its performance using the test set. Assume that with hand-coding we can achieve a 95% accuracy rate. Would your neural network perform better or worse than hand-coding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_val_acc = lstm_history.history['val_acc']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
