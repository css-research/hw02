{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Activation, Dropout, SimpleRNN, LSTM, GRU\n",
    "\n",
    "#Data Setup\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "#Setting up macros (based on question description)\n",
    "MAX_FEATURES = 10000\n",
    "MAX_LEN = 100\n",
    "NUM_TOPICS = 24 \n",
    "all_models=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Data\n",
    "df_train = pd.read_csv('data/congress_train.csv', encoding='ISO-8859-1').dropna()\n",
    "df_test = pd.read_csv('data/congress_test.csv', encoding='ISO-8859-1').dropna()\n",
    "df_valid = pd.read_csv('data/congress_val.csv', encoding='ISO-8859-1').dropna()\n",
    "\n",
    "#Conversion to lists\n",
    "txt_train = [str(word) for word in list(df_train['Title'])]\n",
    "txt_test =  [str(word) for word in list(df_test['Title'])]\n",
    "txt_valid = [str(word) for word in list(df_valid['Title'])] \n",
    "\n",
    "#Conversion to categorical\n",
    "y_train = to_categorical(list(df_train['Major']))\n",
    "y_test = to_categorical(list(df_test['Major']))\n",
    "y_valid = to_categorical(list(df_valid['Major']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(txt_train)\n",
    "train_seq = tokenizer.texts_to_sequences(txt_train)\n",
    "test_seq = tokenizer.texts_to_sequences(txt_test)\n",
    "valid_seq = tokenizer.texts_to_sequences(txt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding\n",
    "X_train = pad_sequences(train_seq, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(test_seq, maxlen=MAX_LEN)\n",
    "X_valid = pad_sequences(valid_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 8s 30us/step - loss: 1.8685 - acc: 0.4872 - val_loss: 1.0707 - val_acc: 0.7238\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 7s 26us/step - loss: 0.8441 - acc: 0.7789 - val_loss: 0.7402 - val_acc: 0.8032\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 9s 34us/step - loss: 0.6587 - acc: 0.8222 - val_loss: 0.6587 - val_acc: 0.8222\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 10s 35us/step - loss: 0.5856 - acc: 0.8410 - val_loss: 0.6211 - val_acc: 0.8329\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 12s 41us/step - loss: 0.5414 - acc: 0.8521 - val_loss: 0.6011 - val_acc: 0.8389\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 11s 39us/step - loss: 0.5096 - acc: 0.8602 - val_loss: 0.5895 - val_acc: 0.8436\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.4850 - acc: 0.8662 - val_loss: 0.5814 - val_acc: 0.8458\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 8s 30us/step - loss: 0.4643 - acc: 0.8718 - val_loss: 0.5772 - val_acc: 0.8479\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.4468 - acc: 0.8766 - val_loss: 0.5759 - val_acc: 0.8479\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.4315 - acc: 0.8809 - val_loss: 0.5738 - val_acc: 0.8494\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.4178 - acc: 0.8846 - val_loss: 0.5736 - val_acc: 0.8492\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 9s 34us/step - loss: 0.4055 - acc: 0.8877 - val_loss: 0.5738 - val_acc: 0.8507\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3945 - acc: 0.8911 - val_loss: 0.5757 - val_acc: 0.8516\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.3844 - acc: 0.8935 - val_loss: 0.5773 - val_acc: 0.8509\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.3749 - acc: 0.8960 - val_loss: 0.5823 - val_acc: 0.8502\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.3664 - acc: 0.8980 - val_loss: 0.5825 - val_acc: 0.8513\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.3586 - acc: 0.9005 - val_loss: 0.5864 - val_acc: 0.8515\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3511 - acc: 0.9024 - val_loss: 0.5892 - val_acc: 0.8515\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.3443 - acc: 0.9040 - val_loss: 0.5929 - val_acc: 0.8519\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3379 - acc: 0.9058 - val_loss: 0.5968 - val_acc: 0.8522\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.3320 - acc: 0.9075 - val_loss: 0.6007 - val_acc: 0.8510\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3263 - acc: 0.9084 - val_loss: 0.6043 - val_acc: 0.8517\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3211 - acc: 0.9102 - val_loss: 0.6095 - val_acc: 0.8512\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.3162 - acc: 0.9114 - val_loss: 0.6134 - val_acc: 0.8507\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3115 - acc: 0.9128 - val_loss: 0.6181 - val_acc: 0.8512\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3070 - acc: 0.9139 - val_loss: 0.6218 - val_acc: 0.8511\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.3029 - acc: 0.9153 - val_loss: 0.6263 - val_acc: 0.8512\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.2989 - acc: 0.9162 - val_loss: 0.6312 - val_acc: 0.8502\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 10s 34us/step - loss: 0.2952 - acc: 0.9173 - val_loss: 0.6361 - val_acc: 0.8499\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 10s 36us/step - loss: 0.2917 - acc: 0.9181 - val_loss: 0.6415 - val_acc: 0.8487\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 9s 34us/step - loss: 0.2883 - acc: 0.9190 - val_loss: 0.6459 - val_acc: 0.8494\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2851 - acc: 0.9199 - val_loss: 0.6526 - val_acc: 0.8486\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 9s 34us/step - loss: 0.2820 - acc: 0.9207 - val_loss: 0.6554 - val_acc: 0.8486\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2791 - acc: 0.9211 - val_loss: 0.6620 - val_acc: 0.8478\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2761 - acc: 0.9221 - val_loss: 0.6653 - val_acc: 0.8479\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2735 - acc: 0.9226 - val_loss: 0.6701 - val_acc: 0.8475\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2709 - acc: 0.9234 - val_loss: 0.6743 - val_acc: 0.8481\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2685 - acc: 0.9241 - val_loss: 0.6801 - val_acc: 0.8467\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2661 - acc: 0.9248 - val_loss: 0.6855 - val_acc: 0.8467\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2639 - acc: 0.9253 - val_loss: 0.6897 - val_acc: 0.8466\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 8s 30us/step - loss: 0.2617 - acc: 0.9258 - val_loss: 0.6949 - val_acc: 0.8466\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2595 - acc: 0.9265 - val_loss: 0.7015 - val_acc: 0.8447\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 9s 33us/step - loss: 0.2576 - acc: 0.9268 - val_loss: 0.7046 - val_acc: 0.8452\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 9s 32us/step - loss: 0.2555 - acc: 0.9272 - val_loss: 0.7092 - val_acc: 0.8454\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2536 - acc: 0.9275 - val_loss: 0.7137 - val_acc: 0.8456\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2520 - acc: 0.9283 - val_loss: 0.7190 - val_acc: 0.8443\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2501 - acc: 0.9289 - val_loss: 0.7249 - val_acc: 0.8439\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2484 - acc: 0.9293 - val_loss: 0.7281 - val_acc: 0.8440\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 9s 31us/step - loss: 0.2469 - acc: 0.9296 - val_loss: 0.7335 - val_acc: 0.8439\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 8s 30us/step - loss: 0.2453 - acc: 0.9299 - val_loss: 0.7379 - val_acc: 0.8436\n"
     ]
    }
   ],
   "source": [
    "#Estimate a basic feed-forward network\n",
    "feedfwd = Sequential()\n",
    "feedfwd.add(Embedding(10000, 25, input_length=100))\n",
    "feedfwd.add(Flatten())\n",
    "feedfwd.add(Dense(24, activation='softmax'))\n",
    "feedfwd.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "result_feedfwd = feedfwd.fit(X_train, y_train, \n",
    "                             validation_data=(X_valid,y_valid), \n",
    "                             epochs=50, \n",
    "                             batch_size=512)\n",
    "all_models.append(result_feedfwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 2.5129 - acc: 0.2573 - val_loss: 2.2196 - val_acc: 0.3380\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 29s 104us/step - loss: 1.9792 - acc: 0.4245 - val_loss: 1.9906 - val_acc: 0.4290\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 1.7124 - acc: 0.5175 - val_loss: 1.6696 - val_acc: 0.5361\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 29s 104us/step - loss: 1.5501 - acc: 0.5709 - val_loss: 1.5678 - val_acc: 0.5676\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 30s 109us/step - loss: 1.4305 - acc: 0.6079 - val_loss: 1.4606 - val_acc: 0.6005\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 31s 112us/step - loss: 1.3313 - acc: 0.6373 - val_loss: 1.3893 - val_acc: 0.6244\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 30s 107us/step - loss: 1.2452 - acc: 0.6628 - val_loss: 1.4083 - val_acc: 0.6178\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 27s 99us/step - loss: 1.1763 - acc: 0.6831 - val_loss: 1.4197 - val_acc: 0.6203\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 1.1177 - acc: 0.6998 - val_loss: 1.2377 - val_acc: 0.6706\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 1.0659 - acc: 0.7143 - val_loss: 1.2641 - val_acc: 0.6640\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 29s 105us/step - loss: 1.0215 - acc: 0.7270 - val_loss: 1.1829 - val_acc: 0.6872\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 34s 122us/step - loss: 0.9870 - acc: 0.7369 - val_loss: 1.1350 - val_acc: 0.7012\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 32s 114us/step - loss: 0.9497 - acc: 0.7474 - val_loss: 1.1995 - val_acc: 0.68371s - loss: 0.9492 \n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 30s 106us/step - loss: 0.9213 - acc: 0.7557 - val_loss: 1.1054 - val_acc: 0.7125\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.8974 - acc: 0.7627 - val_loss: 1.1771 - val_acc: 0.6903\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.8718 - acc: 0.7699 - val_loss: 1.0761 - val_acc: 0.7205\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.8511 - acc: 0.7762 - val_loss: 1.0755 - val_acc: 0.7211\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 0.8320 - acc: 0.7816 - val_loss: 1.2345 - val_acc: 0.6782\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 30s 106us/step - loss: 0.8158 - acc: 0.7866 - val_loss: 1.2778 - val_acc: 0.6747\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 0.7989 - acc: 0.7912 - val_loss: 1.0586 - val_acc: 0.7262\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 31s 113us/step - loss: 0.7838 - acc: 0.7951 - val_loss: 1.0444 - val_acc: 0.7304\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.7711 - acc: 0.7983 - val_loss: 1.0114 - val_acc: 0.7408\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 29s 105us/step - loss: 0.7591 - acc: 0.8019 - val_loss: 1.0729 - val_acc: 0.7237\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 32s 113us/step - loss: 0.7471 - acc: 0.8046 - val_loss: 1.0929 - val_acc: 0.7180\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 30s 108us/step - loss: 0.7413 - acc: 0.8067 - val_loss: 0.9997 - val_acc: 0.7468\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.7340 - acc: 0.8087 - val_loss: 1.0181 - val_acc: 0.7429\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 29s 104us/step - loss: 0.7236 - acc: 0.8112 - val_loss: 1.0582 - val_acc: 0.7300\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 29s 105us/step - loss: 0.7126 - acc: 0.8143 - val_loss: 1.0117 - val_acc: 0.7454\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 30s 108us/step - loss: 1.1531 - acc: 0.6883 - val_loss: 1.4087 - val_acc: 0.6278\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 28s 100us/step - loss: 1.0047 - acc: 0.7263 - val_loss: 1.1500 - val_acc: 0.6958\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.9340 - acc: 0.7468 - val_loss: 1.0956 - val_acc: 0.7137\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 34s 122us/step - loss: 0.9049 - acc: 0.7554 - val_loss: 0.9920 - val_acc: 0.7444\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 0.7550 - acc: 0.8014 - val_loss: 0.9784 - val_acc: 0.7491\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 30s 107us/step - loss: 0.7096 - acc: 0.8139 - val_loss: 0.9688 - val_acc: 0.7539\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 33s 118us/step - loss: 0.7009 - acc: 0.8168 - val_loss: 1.0010 - val_acc: 0.7440\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 31s 112us/step - loss: 0.6899 - acc: 0.8202 - val_loss: 0.9633 - val_acc: 0.7573\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.6746 - acc: 0.8239 - val_loss: 0.9695 - val_acc: 0.7563\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.6644 - acc: 0.8274 - val_loss: 0.9691 - val_acc: 0.7577\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 30s 108us/step - loss: 0.9493 - acc: 0.7446 - val_loss: 1.0970 - val_acc: 0.7145\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.6936 - acc: 0.8177 - val_loss: 0.9600 - val_acc: 0.7581\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 29s 102us/step - loss: 0.7113 - acc: 0.8143 - val_loss: 1.7729 - val_acc: 0.5481\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 28s 101us/step - loss: 0.9019 - acc: 0.7554 - val_loss: 0.9792 - val_acc: 0.7509\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 29s 104us/step - loss: 0.6637 - acc: 0.8263 - val_loss: 0.9549 - val_acc: 0.7613\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 28s 100us/step - loss: 0.6426 - acc: 0.8330 - val_loss: 0.9822 - val_acc: 0.7546\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 28s 100us/step - loss: 0.6330 - acc: 0.8351 - val_loss: 0.9829 - val_acc: 0.7564\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.6256 - acc: 0.8375 - val_loss: 0.9725 - val_acc: 0.7573\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 29s 103us/step - loss: 0.6187 - acc: 0.8393 - val_loss: 1.0244 - val_acc: 0.7459\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 28s 102us/step - loss: 0.6144 - acc: 0.8402 - val_loss: 0.9645 - val_acc: 0.7610\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 28s 99us/step - loss: 0.6227 - acc: 0.8386 - val_loss: 1.0099 - val_acc: 0.7493\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 29s 105us/step - loss: 0.6048 - acc: 0.8432 - val_loss: 0.9895 - val_acc: 0.7551\n"
     ]
    }
   ],
   "source": [
    "#Estimate a recurrent neural network (RNN) with a layer_simple_rnn\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(10000, 20, input_length=100))\n",
    "rnn.add(SimpleRNN(20))\n",
    "rnn.add(Dense(24, activation='softmax'))\n",
    "rnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_rnn = rnn.fit(X_train, y_train,\n",
    "                     validation_data=(X_valid,y_valid),\n",
    "                     epochs=50,\n",
    "                     batch_size=512)\n",
    "all_models.append(result_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 63s 228us/step - loss: 2.2371 - acc: 0.3366 - val_loss: 1.9794 - val_acc: 0.4285\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 58s 209us/step - loss: 1.3607 - acc: 0.6303 - val_loss: 1.1947 - val_acc: 0.6850\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 60s 214us/step - loss: 1.0221 - acc: 0.7397 - val_loss: 1.0216 - val_acc: 0.7390\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 58s 209us/step - loss: 0.8551 - acc: 0.7884 - val_loss: 0.8465 - val_acc: 0.7906\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 60s 214us/step - loss: 0.7529 - acc: 0.8128 - val_loss: 0.7781 - val_acc: 0.8038\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 67s 240us/step - loss: 0.6897 - acc: 0.8267 - val_loss: 0.7419 - val_acc: 0.8119\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 65s 233us/step - loss: 0.6455 - acc: 0.8355 - val_loss: 0.7099 - val_acc: 0.8181\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 63s 226us/step - loss: 0.6123 - acc: 0.8426 - val_loss: 0.6642 - val_acc: 0.8302\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 63s 226us/step - loss: 0.5872 - acc: 0.8478 - val_loss: 0.6774 - val_acc: 0.8249\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.5671 - acc: 0.8519 - val_loss: 0.6425 - val_acc: 0.8328\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 63s 227us/step - loss: 0.5510 - acc: 0.8548 - val_loss: 0.6403 - val_acc: 0.8321\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.5363 - acc: 0.8580 - val_loss: 0.6308 - val_acc: 0.8350\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 62s 222us/step - loss: 0.5243 - acc: 0.8609 - val_loss: 0.6170 - val_acc: 0.8383\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 62s 221us/step - loss: 0.5136 - acc: 0.8628 - val_loss: 0.6128 - val_acc: 0.8390\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 62s 221us/step - loss: 0.5038 - acc: 0.8650 - val_loss: 0.6148 - val_acc: 0.8389\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.4950 - acc: 0.8670 - val_loss: 0.6137 - val_acc: 0.8397\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 62s 223us/step - loss: 0.4868 - acc: 0.8688 - val_loss: 0.6020 - val_acc: 0.8401\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.4789 - acc: 0.8706 - val_loss: 0.5955 - val_acc: 0.8435\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 61s 218us/step - loss: 0.4718 - acc: 0.8725 - val_loss: 0.5913 - val_acc: 0.8446\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.4650 - acc: 0.8740 - val_loss: 0.5928 - val_acc: 0.8421\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.4588 - acc: 0.8756 - val_loss: 0.5891 - val_acc: 0.8439\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.4527 - acc: 0.8767 - val_loss: 0.5969 - val_acc: 0.8426\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 60s 217us/step - loss: 0.4466 - acc: 0.8781 - val_loss: 0.5953 - val_acc: 0.8444\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 60s 217us/step - loss: 0.4414 - acc: 0.8790 - val_loss: 0.5707 - val_acc: 0.8502\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.4357 - acc: 0.8807 - val_loss: 0.5869 - val_acc: 0.8461\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 61s 218us/step - loss: 0.4315 - acc: 0.8815 - val_loss: 0.5671 - val_acc: 0.8514\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 63s 224us/step - loss: 0.4274 - acc: 0.8824 - val_loss: 0.5915 - val_acc: 0.8421\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.4229 - acc: 0.8836 - val_loss: 0.5680 - val_acc: 0.8512\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 61s 218us/step - loss: 0.4186 - acc: 0.8847 - val_loss: 0.5648 - val_acc: 0.8526\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.4149 - acc: 0.8857 - val_loss: 0.5702 - val_acc: 0.8508\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.4109 - acc: 0.8866 - val_loss: 0.5669 - val_acc: 0.8507\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 62s 222us/step - loss: 0.4076 - acc: 0.8873 - val_loss: 0.5622 - val_acc: 0.8534\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 60s 216us/step - loss: 0.4042 - acc: 0.8883 - val_loss: 0.5656 - val_acc: 0.8518\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 60s 216us/step - loss: 0.4013 - acc: 0.8889 - val_loss: 0.5752 - val_acc: 0.8493\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 59s 213us/step - loss: 0.3975 - acc: 0.8900 - val_loss: 0.5710 - val_acc: 0.8508\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 62s 221us/step - loss: 0.3951 - acc: 0.8905 - val_loss: 0.5610 - val_acc: 0.8533\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 60s 217us/step - loss: 0.3922 - acc: 0.8913 - val_loss: 0.5682 - val_acc: 0.8517\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.3891 - acc: 0.8920 - val_loss: 0.5724 - val_acc: 0.8508\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 61s 218us/step - loss: 0.3864 - acc: 0.8931 - val_loss: 0.5615 - val_acc: 0.8538\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 61s 221us/step - loss: 0.3843 - acc: 0.8936 - val_loss: 0.5681 - val_acc: 0.8524\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 61s 217us/step - loss: 0.3816 - acc: 0.8940 - val_loss: 0.5701 - val_acc: 0.8514\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 61s 221us/step - loss: 0.3795 - acc: 0.8942 - val_loss: 0.5695 - val_acc: 0.8523\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.3773 - acc: 0.8950 - val_loss: 0.5678 - val_acc: 0.8526\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 62s 223us/step - loss: 0.3750 - acc: 0.8958 - val_loss: 0.5612 - val_acc: 0.8538\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 61s 218us/step - loss: 0.3727 - acc: 0.8963 - val_loss: 0.5709 - val_acc: 0.8523\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 61s 221us/step - loss: 0.3706 - acc: 0.8971 - val_loss: 0.5690 - val_acc: 0.8527\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 0.3685 - acc: 0.8973 - val_loss: 0.5687 - val_acc: 0.8528\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 61s 221us/step - loss: 0.3666 - acc: 0.8980 - val_loss: 0.5649 - val_acc: 0.8537\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 62s 221us/step - loss: 0.3646 - acc: 0.8985 - val_loss: 0.5707 - val_acc: 0.8525\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 61s 220us/step - loss: 0.3633 - acc: 0.8991 - val_loss: 0.5665 - val_acc: 0.8534\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(10000, 20, input_length=100))\n",
    "lstm.add(LSTM(20))\n",
    "lstm.add(Dense(24, activation='softmax'))\n",
    "lstm.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "result_lstm = lstm.fit(X_train, y_train,\n",
    "                       validation_data=(X_valid,y_valid),\n",
    "                       epochs=50,\n",
    "                       batch_size=512)\n",
    "all_models.append(result_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 65s 233us/step - loss: 2.5482 - acc: 0.2022 - val_loss: 2.2541 - val_acc: 0.2825\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 68s 243us/step - loss: 1.9500 - acc: 0.4267 - val_loss: 1.6474 - val_acc: 0.5470\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 61s 219us/step - loss: 1.4306 - acc: 0.6269 - val_loss: 1.2593 - val_acc: 0.6863\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 72s 257us/step - loss: 1.1206 - acc: 0.7281 - val_loss: 1.1146 - val_acc: 0.7258\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 73s 263us/step - loss: 0.9441 - acc: 0.7694 - val_loss: 0.9395 - val_acc: 0.7672\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 63s 226us/step - loss: 0.8258 - acc: 0.7952 - val_loss: 0.8216 - val_acc: 0.7980\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.7458 - acc: 0.8148 - val_loss: 0.7686 - val_acc: 0.8080\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 63s 228us/step - loss: 0.6886 - acc: 0.8272 - val_loss: 0.7253 - val_acc: 0.8158\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 64s 230us/step - loss: 0.6459 - acc: 0.8357 - val_loss: 0.6863 - val_acc: 0.8252\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 62s 221us/step - loss: 0.6121 - acc: 0.8426 - val_loss: 0.6715 - val_acc: 0.8274\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 63s 225us/step - loss: 0.5855 - acc: 0.8475 - val_loss: 0.6553 - val_acc: 0.8285\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 64s 229us/step - loss: 0.5644 - acc: 0.8522 - val_loss: 0.6432 - val_acc: 0.8331\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.5467 - acc: 0.8560 - val_loss: 0.6186 - val_acc: 0.8390\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.5311 - acc: 0.8591 - val_loss: 0.6080 - val_acc: 0.8409\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 65s 233us/step - loss: 0.5179 - acc: 0.8622 - val_loss: 0.6043 - val_acc: 0.8421\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 65s 234us/step - loss: 0.5058 - acc: 0.8646 - val_loss: 0.5963 - val_acc: 0.8428\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 70s 251us/step - loss: 0.4948 - acc: 0.8668 - val_loss: 0.5911 - val_acc: 0.8458\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 71s 256us/step - loss: 0.4855 - acc: 0.8693 - val_loss: 0.5853 - val_acc: 0.8453\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 72s 258us/step - loss: 0.4766 - acc: 0.8712 - val_loss: 0.5854 - val_acc: 0.8467\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 66s 236us/step - loss: 0.4685 - acc: 0.8731 - val_loss: 0.5820 - val_acc: 0.8470\n",
      "Epoch 21/50\n",
      "278612/278612 [==============================] - 60s 214us/step - loss: 0.4612 - acc: 0.8749 - val_loss: 0.5798 - val_acc: 0.8477\n",
      "Epoch 22/50\n",
      "278612/278612 [==============================] - 73s 261us/step - loss: 0.4542 - acc: 0.8765 - val_loss: 0.5715 - val_acc: 0.8486\n",
      "Epoch 23/50\n",
      "278612/278612 [==============================] - 60s 216us/step - loss: 0.4475 - acc: 0.8786 - val_loss: 0.5758 - val_acc: 0.8498\n",
      "Epoch 24/50\n",
      "278612/278612 [==============================] - 66s 236us/step - loss: 0.4411 - acc: 0.8798 - val_loss: 0.5729 - val_acc: 0.8492\n",
      "Epoch 25/50\n",
      "278612/278612 [==============================] - 65s 233us/step - loss: 0.4355 - acc: 0.8813 - val_loss: 0.5665 - val_acc: 0.8523\n",
      "Epoch 26/50\n",
      "278612/278612 [==============================] - 64s 229us/step - loss: 0.4302 - acc: 0.8826 - val_loss: 0.5678 - val_acc: 0.8515\n",
      "Epoch 27/50\n",
      "278612/278612 [==============================] - 64s 229us/step - loss: 0.4249 - acc: 0.8838 - val_loss: 0.5714 - val_acc: 0.8509\n",
      "Epoch 28/50\n",
      "278612/278612 [==============================] - 65s 234us/step - loss: 0.4197 - acc: 0.8853 - val_loss: 0.5711 - val_acc: 0.8516\n",
      "Epoch 29/50\n",
      "278612/278612 [==============================] - 62s 224us/step - loss: 0.4154 - acc: 0.8863 - val_loss: 0.5645 - val_acc: 0.8529\n",
      "Epoch 30/50\n",
      "278612/278612 [==============================] - 65s 234us/step - loss: 0.4107 - acc: 0.8877 - val_loss: 0.5646 - val_acc: 0.8522\n",
      "Epoch 31/50\n",
      "278612/278612 [==============================] - 60s 217us/step - loss: 0.4066 - acc: 0.8889 - val_loss: 0.5662 - val_acc: 0.8534\n",
      "Epoch 32/50\n",
      "278612/278612 [==============================] - 64s 228us/step - loss: 0.4024 - acc: 0.8900 - val_loss: 0.5677 - val_acc: 0.8523\n",
      "Epoch 33/50\n",
      "278612/278612 [==============================] - 67s 240us/step - loss: 0.3983 - acc: 0.8909 - val_loss: 0.5636 - val_acc: 0.8534\n",
      "Epoch 34/50\n",
      "278612/278612 [==============================] - 66s 235us/step - loss: 0.3948 - acc: 0.8918 - val_loss: 0.5635 - val_acc: 0.8543\n",
      "Epoch 35/50\n",
      "278612/278612 [==============================] - 64s 230us/step - loss: 0.3914 - acc: 0.8927 - val_loss: 0.5615 - val_acc: 0.8536\n",
      "Epoch 36/50\n",
      "278612/278612 [==============================] - 63s 226us/step - loss: 0.3878 - acc: 0.8939 - val_loss: 0.5673 - val_acc: 0.8525\n",
      "Epoch 37/50\n",
      "278612/278612 [==============================] - 63s 225us/step - loss: 0.3849 - acc: 0.8947 - val_loss: 0.5623 - val_acc: 0.8539\n",
      "Epoch 38/50\n",
      "278612/278612 [==============================] - 64s 229us/step - loss: 0.3814 - acc: 0.8955 - val_loss: 0.5633 - val_acc: 0.8541\n",
      "Epoch 39/50\n",
      "278612/278612 [==============================] - 64s 230us/step - loss: 0.3782 - acc: 0.8966 - val_loss: 0.5638 - val_acc: 0.8536\n",
      "Epoch 40/50\n",
      "278612/278612 [==============================] - 71s 257us/step - loss: 0.3752 - acc: 0.8967 - val_loss: 0.5703 - val_acc: 0.8531\n",
      "Epoch 41/50\n",
      "278612/278612 [==============================] - 67s 239us/step - loss: 0.3724 - acc: 0.8978 - val_loss: 0.5630 - val_acc: 0.8556\n",
      "Epoch 42/50\n",
      "278612/278612 [==============================] - 68s 243us/step - loss: 0.3698 - acc: 0.8984 - val_loss: 0.5636 - val_acc: 0.8545\n",
      "Epoch 43/50\n",
      "278612/278612 [==============================] - 67s 239us/step - loss: 0.3666 - acc: 0.8994 - val_loss: 0.5700 - val_acc: 0.8538\n",
      "Epoch 44/50\n",
      "278612/278612 [==============================] - 67s 239us/step - loss: 0.3641 - acc: 0.9002 - val_loss: 0.5655 - val_acc: 0.8554\n",
      "Epoch 45/50\n",
      "278612/278612 [==============================] - 66s 237us/step - loss: 0.3616 - acc: 0.9009 - val_loss: 0.5707 - val_acc: 0.8543\n",
      "Epoch 46/50\n",
      "278612/278612 [==============================] - 63s 227us/step - loss: 0.3595 - acc: 0.9012 - val_loss: 0.5721 - val_acc: 0.8538\n",
      "Epoch 47/50\n",
      "278612/278612 [==============================] - 65s 233us/step - loss: 0.3567 - acc: 0.9021 - val_loss: 0.5680 - val_acc: 0.8549\n",
      "Epoch 48/50\n",
      "278612/278612 [==============================] - 68s 244us/step - loss: 0.3545 - acc: 0.9025 - val_loss: 0.5660 - val_acc: 0.8555\n",
      "Epoch 49/50\n",
      "278612/278612 [==============================] - 69s 247us/step - loss: 0.3520 - acc: 0.9035 - val_loss: 0.5670 - val_acc: 0.8559\n",
      "Epoch 50/50\n",
      "278612/278612 [==============================] - 65s 234us/step - loss: 0.3498 - acc: 0.9039 - val_loss: 0.5678 - val_acc: 0.8553\n"
     ]
    }
   ],
   "source": [
    "#Estimate an RNN with a GRU layer\n",
    "gru = Sequential()\n",
    "gru.add(Embedding(10000, 20, input_length=100))\n",
    "gru.add(GRU(20))\n",
    "gru.add(Dense(24, activation='softmax'))\n",
    "gru.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "result_gru = gru.fit(X_train, y_train, \n",
    "                     validation_data=(X_valid,y_valid), \n",
    "                     epochs=50, \n",
    "                     batch_size=512)\n",
    "all_models.append(result_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network vs Hand-Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_dropout = Sequential()\n",
    "rnn_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_dropout.add(SimpleRNN(25, dropout=0.2))\n",
    "rnn_dropout.add(Dense(24, activation='softmax'))\n",
    "rnn_dropout.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "result_rnn_dropout = rnn_dropout.fit(X_train, y_train,\n",
    "                                     validation_data=(X_valid,y_valid),\n",
    "                                     epochs=25,\n",
    "                                     batch_size=512)\n",
    "all_models.append(result_rnn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dropout = Sequential()\n",
    "lstm_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "lstm_dropout.add(LSTM(25, dropout=0.2))\n",
    "lstm_dropout.add(Dense(24, activation='softmax'))\n",
    "lstm_dropout.compile(optimizer='rmsprop',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "result_lstm_dropout = lstm_dropout.fit(X_train, y_train,\n",
    "                                       validation_data=(X_valid,y_valid),\n",
    "                                       epochs=25,\n",
    "                                       batch_size=512)\n",
    "all_models.append(result_lstm_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_dropout = Sequential()\n",
    "gru_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "gru_dropout.add(GRU(25, dropout=0.2))\n",
    "gru_dropout.add(Dense(24, activation='softmax'))\n",
    "gru_dropout.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "result_gru_dropout = gru_dropout.fit(X_train, y_train,\n",
    "                                     validation_data=(X_valid,y_valid),\n",
    "                                     epochs=25,\n",
    "                                     batch_size=512)\n",
    "all_models.append(result_gru_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2layer = Sequential()\n",
    "rnn_2layer.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_2layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_2layer.add(SimpleRNN(25))\n",
    "rnn_2layer.add(Dense(24, activation='softmax'))\n",
    "rnn_2layer.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_rnn_2layer = rnn_2layer.fit(X_train, y_train,\n",
    "                                   validation_data=(X_valid,y_valid),\n",
    "                                   epochs=25,\n",
    "                                   batch_size=512)\n",
    "all_models.append(result_rnn_2layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_3layer = Sequential()\n",
    "rnn_3layer.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_3layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_3layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_3layer.add(SimpleRNN(25))\n",
    "rnn_3layer.add(Dense(24, activation='softmax'))\n",
    "rnn_3layer.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "result_rnn_3layer = rnn_3layer.fit(X_train, y_train,\n",
    "                                   validation_data=(X_valid,y_valid),\n",
    "                                   epochs=25,\n",
    "                                   batch_size=512)\n",
    "all_models.append(result_rnn_3layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(model):   \n",
    "    plt.plot(model.history['val_acc'])\n",
    "\n",
    "def plot_loss:\n",
    "    plt.plot(model.history['val_loss'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Plot- Accuracies\n",
    "for model in all_models:\n",
    "    plot_acc(model)\n",
    "\n",
    "plt.title('Accuracy on the Validation Set for All models Across Epochs')\n",
    "plt.legend(['Basic Feed Forward', 'Basic RNN', \n",
    "            'Basic LSTM','Basic GRU',\n",
    "            'RNN with Dropout', \n",
    "            'LSTM with Dropout','GRU with Dropout',\n",
    "           'RNN with 2 layers', 'RNN with 3 layers'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Plot- Loss\n",
    "for model in all_models:\n",
    "    plot_loss(model)\n",
    "\n",
    "plt.title('Loss on the Validation Set for All models Across Epochs')\n",
    "plt.legend(['Basic Feed Forward', 'Basic RNN', \n",
    "            'Basic LSTM','Basic GRU',\n",
    "            'RNN with Dropout', \n",
    "            'LSTM with Dropout','GRU with Dropout',\n",
    "           'RNN with 2 layers', 'RNN with 3 layers'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per these plot, we can see that the highest performing one is the LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38693/38693 [==============================] - 9s 235us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5719151524522886, 0.8529708215971779]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we use this model on the validation data\n",
    "lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
