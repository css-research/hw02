{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Activation, Dropout, SimpleRNN, LSTM, GRU\n",
    "\n",
    "#Data Setup\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "#Setting up macros (based on question description)\n",
    "MAX_FEATURES = 10000\n",
    "MAX_LEN = 100\n",
    "NUM_TOPICS = 24 \n",
    "all_models=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Data\n",
    "df_train = pd.read_csv('data/congress_train.csv', encoding='ISO-8859-1').dropna()\n",
    "df_test = pd.read_csv('data/congress_test.csv', encoding='ISO-8859-1').dropna()\n",
    "df_valid = pd.read_csv('data/congress_val.csv', encoding='ISO-8859-1').dropna()\n",
    "\n",
    "#Conversion to lists\n",
    "txt_train = [str(word) for word in list(df_train['Title'])]\n",
    "txt_test =  [str(word) for word in list(df_test['Title'])]\n",
    "txt_valid = [str(word) for word in list(df_valid['Title'])] \n",
    "\n",
    "#Conversion to categorical\n",
    "y_train = to_categorical(list(df_train['Major']))\n",
    "y_test = to_categorical(list(df_test['Major']))\n",
    "y_valid = to_categorical(list(df_valid['Major']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(txt_train)\n",
    "train_seq = tokenizer.texts_to_sequences(txt_train)\n",
    "test_seq = tokenizer.texts_to_sequences(txt_test)\n",
    "valid_seq = tokenizer.texts_to_sequences(txt_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding\n",
    "X_train = pad_sequences(train_seq, maxlen=MAX_LEN)\n",
    "X_test = pad_sequences(test_seq, maxlen=MAX_LEN)\n",
    "X_valid = pad_sequences(valid_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 278612 samples, validate on 69649 samples\n",
      "Epoch 1/50\n",
      "278612/278612 [==============================] - 12s 44us/step - loss: 1.8538 - acc: 0.4917 - val_loss: 1.0661 - val_acc: 0.7240\n",
      "Epoch 2/50\n",
      "278612/278612 [==============================] - 11s 41us/step - loss: 0.8410 - acc: 0.7795 - val_loss: 0.7392 - val_acc: 0.8047\n",
      "Epoch 3/50\n",
      "278612/278612 [==============================] - 11s 41us/step - loss: 0.6582 - acc: 0.8225 - val_loss: 0.6585 - val_acc: 0.8257\n",
      "Epoch 4/50\n",
      "278612/278612 [==============================] - 11s 41us/step - loss: 0.5859 - acc: 0.8404 - val_loss: 0.6225 - val_acc: 0.8337\n",
      "Epoch 5/50\n",
      "278612/278612 [==============================] - 12s 44us/step - loss: 0.5422 - acc: 0.8511 - val_loss: 0.6045 - val_acc: 0.8388\n",
      "Epoch 6/50\n",
      "278612/278612 [==============================] - 11s 38us/step - loss: 0.5105 - acc: 0.8593 - val_loss: 0.5914 - val_acc: 0.8417\n",
      "Epoch 7/50\n",
      "278612/278612 [==============================] - 11s 39us/step - loss: 0.4858 - acc: 0.8657 - val_loss: 0.5836 - val_acc: 0.8445\n",
      "Epoch 8/50\n",
      "278612/278612 [==============================] - 12s 42us/step - loss: 0.4653 - acc: 0.8718 - val_loss: 0.5785 - val_acc: 0.8461\n",
      "Epoch 9/50\n",
      "278612/278612 [==============================] - 12s 42us/step - loss: 0.4478 - acc: 0.8761 - val_loss: 0.5757 - val_acc: 0.8478\n",
      "Epoch 10/50\n",
      "278612/278612 [==============================] - 11s 39us/step - loss: 0.4325 - acc: 0.8805 - val_loss: 0.5761 - val_acc: 0.8485\n",
      "Epoch 11/50\n",
      "278612/278612 [==============================] - 11s 41us/step - loss: 0.4190 - acc: 0.8839 - val_loss: 0.5748 - val_acc: 0.8495\n",
      "Epoch 12/50\n",
      "278612/278612 [==============================] - 14s 49us/step - loss: 0.4068 - acc: 0.8871 - val_loss: 0.5758 - val_acc: 0.8495\n",
      "Epoch 13/50\n",
      "278612/278612 [==============================] - 14s 51us/step - loss: 0.3959 - acc: 0.8902 - val_loss: 0.5763 - val_acc: 0.8509\n",
      "Epoch 14/50\n",
      "278612/278612 [==============================] - 13s 47us/step - loss: 0.3856 - acc: 0.8928 - val_loss: 0.5783 - val_acc: 0.8507\n",
      "Epoch 15/50\n",
      "278612/278612 [==============================] - 14s 51us/step - loss: 0.3763 - acc: 0.8951 - val_loss: 0.5818 - val_acc: 0.8505\n",
      "Epoch 16/50\n",
      "278612/278612 [==============================] - 11s 39us/step - loss: 0.3678 - acc: 0.8977 - val_loss: 0.5848 - val_acc: 0.8503\n",
      "Epoch 17/50\n",
      "278612/278612 [==============================] - 12s 43us/step - loss: 0.3597 - acc: 0.9000 - val_loss: 0.5865 - val_acc: 0.8505\n",
      "Epoch 18/50\n",
      "278612/278612 [==============================] - 13s 46us/step - loss: 0.3524 - acc: 0.9017 - val_loss: 0.5905 - val_acc: 0.8508\n",
      "Epoch 19/50\n",
      "278612/278612 [==============================] - 12s 42us/step - loss: 0.3453 - acc: 0.9040 - val_loss: 0.5952 - val_acc: 0.8506\n",
      "Epoch 20/50\n",
      "278612/278612 [==============================] - 13s 47us/step - loss: 0.3389 - acc: 0.9054 - val_loss: 0.5985 - val_acc: 0.8505\n",
      "Epoch 21/50\n",
      "192000/278612 [===================>..........] - ETA: 3s - loss: 0.3286 - acc: 0.9083- ETA: 4s - loss: 0.3280 - ETA: 3s - loss: 0.3285 - acc: 0.908"
     ]
    }
   ],
   "source": [
    "#Estimate a basic feed-forward network\n",
    "feedfwd = Sequential()\n",
    "feedfwd.add(Embedding(10000, 25, input_length=100))\n",
    "feedfwd.add(Flatten())\n",
    "feedfwd.add(Dense(24, activation='softmax'))\n",
    "feedfwd.compile(optimizer='rmsprop', \n",
    "                loss='categorical_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "result_feedfwd = feedfwd.fit(X_train, y_train, \n",
    "                             validation_data=(X_valid,y_valid), \n",
    "                             epochs=50, \n",
    "                             batch_size=512)\n",
    "#all_models.append(result_feedfwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate a recurrent neural network (RNN) with a layer_simple_rnn\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(10000, 20, input_length=100))\n",
    "rnn.add(SimpleRNN(20))\n",
    "rnn.add(Dense(24, activation='softmax'))\n",
    "rnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_rnn = rnn.fit(train_x, train_y, \n",
    "                     validation_data=(valid_x,valid_y),\n",
    "                     epochs=50,\n",
    "                     batch_size=512)\n",
    "all_models.append(result_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(10000, 20, input_length=100))\n",
    "lstm.add(LSTM(20))\n",
    "lstm.add(Dense(24, activation='softmax'))\n",
    "lstm.compile(optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "result_lstm = lstm.fit(train_x, train_y, \n",
    "                       validation_data=(valid_x,valid_y),\n",
    "                       epochs=50,\n",
    "                       batch_size=512)\n",
    "all_models.append(result_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimate an RNN with a GRU layer\n",
    "gru = Sequential()\n",
    "gru.add(Embedding(10000, 20, input_length=100))\n",
    "gru.add(GRU(20))\n",
    "gru.add(Dense(24, activation='softmax'))\n",
    "gru.compile(optimizer='rmsprop',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n",
    "result_gru = gru.fit(train_x, train_y, \n",
    "                     validation_data=(valid_x,valid_y), \n",
    "                     epochs=50, \n",
    "                     batch_size=512)\n",
    "all_models.append(result_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network vs Hand-Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_2layer = Sequential()\n",
    "rnn_2layer.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_2layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_2layer.add(SimpleRNN(25))\n",
    "rnn_2layer.add(Dense(24, activation='softmax'))\n",
    "rnn_2layer.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "result_rnn_2layer = rnn_2layer.fit(X_train, y_train,\n",
    "                                   validation_data=(X_valid,y_valid),\n",
    "                                   epochs=25,\n",
    "                                   batch_size=512)\n",
    "all_models.append(result_rnn_2layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_3layer = Sequential()\n",
    "rnn_3layer.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_3layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_3layer.add(SimpleRNN(25, return_sequences=True))\n",
    "rnn_3layer.add(SimpleRNN(25))\n",
    "rnn_3layer.add(Dense(24, activation='softmax'))\n",
    "rnn_3layer.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "result_rnn_3layer = rnn_3layer.fit(X_train, y_train,\n",
    "                                   validation_data=(X_valid,y_valid),\n",
    "                                   epochs=25,\n",
    "                                   batch_size=512)\n",
    "all_models.append(result_rnn_3layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_dropout = Sequential()\n",
    "rnn_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "rnn_dropout.add(SimpleRNN(25, dropout=0.2))\n",
    "rnn_dropout.add(Dense(24, activation='softmax'))\n",
    "rnn_dropout.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "result_rnn_dropout = rnn_dropout.fit(X_train, y_train,\n",
    "                                     validation_data=(X_valid,y_valid),\n",
    "                                     epochs=25,\n",
    "                                     batch_size=512)\n",
    "all_models.append(result_rnn_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dropout = Sequential()\n",
    "lstm_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "lstm_dropout.add(LSTM(25, dropout=0.2))\n",
    "lstm_dropout.add(Dense(24, activation='softmax'))\n",
    "lstm_dropout.compile(optimizer='rmsprop',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "result_lstm_dropout = lstm_dropout.fit(X_train, y_train,\n",
    "                                       validation_data=(X_valid,y_valid),\n",
    "                                       epochs=25,\n",
    "                                       batch_size=512)\n",
    "all_models.append(result_lstm_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_dropout = Sequential()\n",
    "gru_dropout.add(Embedding(10000, 25, input_length=100))\n",
    "gru_dropout.add(GRU(25, dropout=0.2))\n",
    "gru_dropout.add(Dense(24, activation='softmax'))\n",
    "gru_dropout.compile(optimizer='rmsprop',\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "result_gru_dropout = gru_dropout.fit(X_train, y_train,\n",
    "                                     validation_data=(X_valid,y_valid),\n",
    "                                     epochs=25,\n",
    "                                     batch_size=512)\n",
    "all_models.append(result_gru_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined Plot\n",
    "for model in all_models:\n",
    "    plot_loss(model)\n",
    "\n",
    "plt.title('Loss on the Validation Set for All models Across Epochs')\n",
    "plt.legend(['Basic Feed Forward', 'Basic RNN', \n",
    "            'Basic LSTM','Basic GRU',\n",
    "            'RNN with Dropout', \n",
    "            'LSTM with Dropout','GRU with Dropout',\n",
    "           'RNN with 2 layers, RNN with 3 layers'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
